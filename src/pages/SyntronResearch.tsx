import { useState, useEffect } from "react";
import Navigation from "@/components/Navigation";
import MegaFooter from "@/components/MegaFooter";
import { Link } from "react-router-dom";
import { BlockMath, InlineMath } from 'react-katex';
import 'katex/dist/katex.min.css';

// Reusable ExpandableImage component
const ExpandableImage = ({ src, alt, className = '' }) => {
  const [modalOpen, setModalOpen] = useState(false);
  const [show, setShow] = useState(false);

  useEffect(() => {
    if (modalOpen) {
      setShow(false);
      const timeout = setTimeout(() => setShow(true), 10);
      return () => clearTimeout(timeout);
    } else {
      setShow(false);
    }
  }, [modalOpen]);

  return (
    <>
      <div className="relative group">
        <img
          src={src}
          alt={alt}
          className={className + ' cursor-pointer'}
          onClick={() => setModalOpen(true)}
        />
        <button
          className="absolute top-2 right-2 bg-white/80 rounded-full p-1 opacity-0 group-hover:opacity-100 transition-opacity"
          onClick={(e) => { e.stopPropagation(); setModalOpen(true); }}
          title="Expand"
          type="button"
        >
          <svg width="24" height="24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className="text-gray-700"><path d="M15 3h6v6"/><path d="M9 21H3v-6"/><path d="M21 3l-7.5 7.5"/><path d="M3 21l7.5-7.5"/></svg>
        </button>
      </div>
      {modalOpen && (
        <div
          className={`fixed inset-0 z-50 flex items-center justify-center bg-black/70 transition-opacity duration-300 ${show ? 'opacity-100' : 'opacity-0'}`}
          onClick={() => setModalOpen(false)}
        >
          <img
            src={src}
            alt={alt}
            className={`max-w-3xl max-h-[90vh] rounded transition-opacity duration-300 ${show ? 'opacity-100' : 'opacity-0'}`}
            onClick={(e) => e.stopPropagation()}
          />
          <button className="absolute top-8 right-8 text-white text-3xl font-bold" onClick={() => setModalOpen(false)} title="Close" type="button">&times;</button>
        </div>
      )}
    </>
  );
};

const SyntronResearch = () => {
  return (
    <div className="min-h-screen bg-background">
      <Navigation />
      <div className="pt-6">
        <main className="min-h-screen flex items-center justify-center px-6 md:px-8 py-20">
          <div className="max-w-4xl mx-auto text-left space-y-8">
            <div className="mb-12">
              <Link 
                to="/en/research" 
                className="inline-flex items-center text-lg font-medium"
                style={{ color: '#2a9d8f' }}
              >
                ← Back to Research
              </Link>
            </div>
            
            <div className="text-sm text-muted-foreground font-mono uppercase tracking-wider" style={{ color: '#2a9d8f' }}>
              AUG 23, 2025
            </div>
            <h1 className="text-3xl sm:text-4xl md:text-5xl lg:text-6xl font-normal leading-tight tracking-tight" style={{ color: '#264653' }}>
              From Neurons to Syntrons : the ASI Shift
            </h1>
            <div className="text-base md:text-lg font-inter leading-relaxed max-w-3xl space-y-6 pt-8" style={{ color: '#264653' }}>
              <p>
                Artificial intelligence has long been governed by the tyranny of weights. From perceptrons to deep neural networks, the foundations of AI have revolved around optimizing scalar parameters that connect inputs to outputs. These weights are tuned through gradient descent, forming a brittle scaffolding of statistical associations. This weight-centric paradigm, despite its immense success, has inherent limits: it compresses complexity into static numbers, leaving little room for emergence, meaning, or true adaptive intelligence.
              </p>
              <p>
                The newly invented Syntrons a revolutionary departure from traditional weight-based neural architectures. Syntrons transform artificial intelligence from static parameter optimization into dynamic possibility spaces, enabling genuine emergent thought, counterfactual reasoning, and self-organizing intelligence. This paradigm shift represents a fundamental leap from mechanism to mind.
              </p>
            </div>

            {/* Images */}
            <div className="pt-8 max-w-3xl">
              <div className="flex flex-col md:flex-row items-start gap-8">
                <div className="w-full md:w-2/3">
                  <ExpandableImage
                    src="/images/neuron_chestmnist_evolution_fixed.gif"
                    alt="Syntron Tissue MNIST Two Phase Evolution"
                    className="w-full h-auto object-cover border border-muted"
                  />
                </div>
                <div className="w-full md:w-1/3">
                  <ExpandableImage
                    src="/images/epoch_16_input_heatmap.png"
                    alt="Epoch 16 Input Heatmap"
                    className="w-full h-auto object-cover border border-muted"
                  />
                </div>
              </div>
            </div>

            <div className="text-base md:text-lg font-inter leading-relaxed max-w-3xl space-y-6 pt-8" style={{ color: '#264653' }}>
              <h2 className="text-2xl sm:text-3xl font-bold" style={{ color: '#264653' }}>1. The Collapse of Weight-Centric AI: Foundations and Limitations</h2>
              <p>The dominant paradigm in modern artificial intelligence is built upon a foundational abstraction: the artificial neuron, interconnected through a web of static, scalar weights. The optimization of this vast weight matrix via gradient descent has been the engine of the deep learning revolution, yielding unprecedented results across countless domains. However, to chart a course toward more general and adaptive intelligence, we must first critically examine the inherent architectural limitations of this weight-centric model.</p>

              <h2 className="text-xl sm:text-2xl font-bold" style={{ color: '#264653' }}>1.1 The Tyranny of Scalar Parameters</h2>
              <p>A traditional neural network encodes its entire learned knowledge as a single, high-dimensional point estimate—the final set of optimized weights. Each weight w is a deterministic scalar that represents the fixed strength of a single synaptic connection. This model, while computationally efficient, imposes a rigid structure on the network's function. The operational plan of the network is established once at the end of training and remains static throughout inference. The network learns what to compute, but the way it computes is immutable.</p>
              <p>This approach is a significant simplification of biological neural computation, which exhibits dynamic functional reconfiguration, multi-timescale plasticity, and context-dependent processing. A biological synapse is not a static wire; it is a complex biochemical machine.</p>

              <h2 className="text-4xl font-bold" style={{ color: '#264653' }}>1.2 Inherent Limitations of Weight-Based Systems</h2>
              <p>The reliance on a static weight matrix gives rise to several fundamental challenges that may represent a ceiling on the capabilities of current architectures:</p>

              <p><strong>Lack of Inherent Uncertainty Representation:</strong> A static weight is a point estimate; it does not intrinsically represent the model's uncertainty about that parameter's value. The network can express uncertainty in its final output (e.g., via a softmax distribution), but it lacks a native mechanism to represent uncertainty in its own internal computational steps.</p>

              <p><strong>Static Architecture:</strong> The network's architecture is fixed. Its capacity to learn is fundamentally limited by the number of parameters defined at initialization. It cannot autonomously decide to allocate more computational resources to a particularly complex aspect of a problem.</p>

              <p><strong>Conflation of Knowledge:</strong> Knowledge is conflated into a single scalar. The weight w_ij simultaneously encodes the type of operation (e.g., inhibitory, excitatory) and the magnitude of that operation. There is no factorization of "what to do" from "how strongly to do it."</p>

              <h2 className="text-2xl sm:text-3xl font-bold" style={{ color: '#264653' }}>2. Syntrons: The Architecture of Possibility</h2>
              <p>The Syntron architecture is a direct response to these limitations. It proposes a new foundational unit of computation that replaces the static, scalar weight with a dynamic, probabilistic process.</p>

              <h2 className="text-xl sm:text-2xl font-bold" style={{ color: '#264653' }}>2.1 A Fundamental Paradigm Shift</h2>
              <p>The central thesis of the Syntron is the replacement of the deterministic weight w with a generative model that formulates the connection's function on-the-fly. This is achieved through the factorization of knowledge into two distinct components: a shared vocabulary of operations and a local, learned policy for selecting from that library.</p>
              <p>The connection is transformed from a passive wire into an active computational unit that formulates a new operational plan for every forward pass.</p>

              <h2 className="text-xl sm:text-2xl font-bold" style={{ color: '#264653' }}>2.2 The Primitive Basis: A Shared Computational Vocabulary</h2>
              <p>At the heart of the Syntron architecture is a shared, learnable library of fundamental computational operators called Primitives.</p>

              <p><strong>Definition:</strong> Primitives constitute a finite, discrete, and globally optimized set of basis functions. They represent the universal "computational vocabulary" available to the entire network.</p>

              <p><strong>Generality:</strong> These operators are abstract and domain-agnostic. In a fully connected layer, a primitive might be a simple scalar multiplier. In a convolutional network, a primitive is an entire convolutional kernel. The key is that they are reusable, foundational blocks of computation.</p>

              <p><strong>Function:</strong> This approach allows the network to learn a compressed, globally-optimized vocabulary of powerful operations, rather than learning millions of unique and potentially redundant parameters from scratch.</p>

              <div className="w-full">
                <ExpandableImage
                  src="/images/syntron_comprehensive.png"
                  alt="Syntron comprehensive"
                  className="w-full h-auto object-cover border border-muted"
                />
              </div>

              <h2 className="text-xl sm:text-2xl font-bold" style={{ color: '#264653' }}>2.3 The Mixture Distribution: A Learned Policy of Possibilities</h2>
              <p>This is where the local, adaptive intelligence of the Syntron is encoded. Every single connection in the network learns its own unique policy for choosing from the shared vocabulary of Primitives.</p>

              <p><strong>Mechanism:</strong> For each output neuron, the connection's mechanism maintains a vector of logits, representing its unnormalized preference for each available primitive. These logits are passed through a temperature-controlled softmax function to yield a valid categorical probability distribution.</p>

              <p><strong>The Result:</strong> The output of this process is a series of possibilities—a mixture_weights tensor. For any given input, the connection computes a posterior belief over the utility of each primitive. The connection is no longer defined by a single number, but by a flexible, probabilistic recipe for action.</p>

              <div className="w-full">
                <ExpandableImage
                  src="/images/Smultiplesnapped.png"
                  alt="Syntron multiple coupling hypotheses"
                  className="w-full h-auto object-cover border border-muted"
                />
              </div>

              <h2 className="text-xl sm:text-2xl font-bold" style={{ color: '#264653' }}>2.4 The Emergent Operator: Computation as Expectation</h2>
              <p>With this probabilistic framework, the final operator for a layer is no longer a stored parameter. It is a dynamic, emergent property computed for every forward pass. The effective operator is the expectation of the primitive functions, weighted by their selection probabilities from the mixture distribution. This means a single Syntron network can behave like an ensemble of countless different networks, instantiating the most appropriate "sub-network" for each specific input it encounters.</p>

              <h2 className="text-2xl sm:text-3xl font-bold pt-8" style={{ color: '#264653' }}>3. Enabling Cognitive Functions and Self-Modification</h2>
              <p>The probabilistic and dynamic nature of Syntrons enables a suite of advanced features that are intractable in traditional static networks.</p>

              <h2 className="text-xl sm:text-2xl font-bold" style={{ color: '#264653' }}>3.1 Explicit Working Memory</h2>
              <p>Each Syntron connection maintains a buffer of its recent activity, storing a temporal trace of the most recently used mixture weights. This provides the network with a short-term memory of its own internal decisions. This explicit memory trace is a crucial component for enabling more complex behaviors, offering a window into the temporal dynamics of the network's decision-making process and forming a basis for higher-order adaptation.</p>

              <h2 className="text-xl sm:text-2xl font-bold" style={{ color: '#264653' }}>3.2 Dynamic Structural Plasticity: A Network That Learns to Grow</h2>
              <p>A key feature of my research is a mechanism that allows the Syntron network to modify its own architecture during training. This is a behavior driven by the network's own internal state.</p>

              <p><strong>The Trigger - "Tail Interest":</strong> The network constantly monitors its own computational state via a metric I call "Tail Interest." This heuristic quantifies a state of "productive confusion" by combining the entropy of its mixture distributions with the variance of its activations.</p>

              <p><strong>The "Birth" Event:</strong> When "Tail Interest" surpasses a set threshold, the network's control mechanism determines its current computational vocabulary is insufficient. It triggers a "birth" event, adding a new primitive to its shared basis. This is a direct, structural modification of the model's hypothesis space.</p>

              <p><strong>Stable Adaptation:</strong> To ensure stability, this process is carefully managed. The new primitive is "warm-started" based on the statistical mean of the existing, trained primitives, and the internal state of the training optimizer is programmatically reset for the newly expanded parameter tensors.</p>

              <p>This represents a paradigm shift from designing a static architecture to designing a system with the meta-ability to design itself, growing in complexity only when its own internal state indicates a need for greater representational power.</p>

              <h2 className="text-2xl sm:text-3xl font-bold pt-8" style={{ color: '#264653' }}>4. Philosophical and Theoretical Implications</h2>
              <p>The move from a static, weight-centric paradigm to a dynamic, probabilistic one has profound implications that extend beyond engineering.</p>

              <p>The Syntron architecture suggests a model of intelligence where learning is not merely the optimization of a fixed function, but the development of a language of computation. The primitives form the phonemes and morphemes—the basic, meaning-carrying units—while the mixture distributions represent the syntax and semantics—the rules for combining these units into coherent computational "sentences."</p>

              <p>This reframes the nature of generalization. A traditional network generalizes by finding a smooth interpolation function in a high-dimensional space. A Syntron network may generalize by discovering that a new, unseen problem can be solved by a novel composition of its existing, learned primitives. It learns to "reason" with its computational vocabulary, a process arguably closer to human problem-solving.</p>

              <p>Furthermore, the introduction of self-monitoring metrics like "Tail Interest" and the resulting structural plasticity represents a nascent form of computational introspection. The network is not a passive substrate being molded by data; it is an active system that evaluates its own performance and limitations, and takes explicit action to address them. This introduces a feedback loop not just at the level of weights, but at the level of the architecture itself, a foundational step toward more autonomous and truly adaptive learning systems.</p>

              <div className="flex flex-col md:flex-row space-y-4 md:space-y-0 md:space-x-4 pt-4">
                  <div className="w-full md:w-1/2">
                    <ExpandableImage
                      src="/images/neuronsorg.jpg"
                      alt="Neurons organization"
                      className="w-full h-auto object-cover border border-muted"
                    />
                  </div>
                  <div className="w-full md:w-1/2">
                    <ExpandableImage
                      src="/images/neuronsorg.avif"
                      alt="Neurons organization in AVIF format"
                      className="w-full h-auto object-cover border border-muted"
                    />
                  </div>
              </div>

              <h2 className="text-2xl sm:text-3xl font-bold pt-8" style={{ color: '#264653' }}>5. Does More Syntrons = Smarter AI?</h2>
              <p>This is the critical and nuanced research question that emerges from this work. The answer is not a simple "yes," just as an African elephant with 257 billion neurons is not inherently smarter than a human with 86 billion neurons. Intelligence emerges from the efficiency, structure, and dynamic interplay of the underlying components, not just their raw number.</p>

              <ExpandableImage
                  src="/images/The-human-brain-is-not-the-largest-Brains-of-a-human-and-of-an-African-elephant-are.png"
                  alt="Comparison of human and elephant brains"
                  className="w-full h-auto object-cover border border-muted"
                />

              <p>Simply increasing the number of primitives (max_primitives) from the outset does not necessarily lead to a "smarter" AI. In fact, providing too large a vocabulary can make the learning problem harder, increasing the search space for the mixture distributions and potentially leading to overfitting or slower convergence.</p>

              <p>The intelligence of the Syntron system does not derive from having a massive number of primitives, but from the elegance of the interplay between the primitives and the policies. A "smarter" Syntron network would be one that:</p>

              <ul className="list-disc list-inside space-y-2 pl-4">
                <li><strong>Learns a Parsimonious and Powerful Primitive Basis:</strong> It discovers a small set of highly effective, orthogonal, and reusable computational primitives that form a powerful basis for a wide range of problems.</li>
                <li><strong>Develops Sophisticated and Context-Sensitive Policies:</strong> Its mixture distributions are sharp and confident when the task is clear, but appropriately diffuse and uncertain when faced with ambiguous input, effectively modeling its own confidence.</li>
                <li><strong>Exhibits Efficient Plasticity:</strong> It does not grow indiscriminately. It only adds new primitives when its "Tail Interest" indicates a genuine failure of its existing representational capacity. An intelligent system is not just about the size of its vocabulary, but about knowing when it needs to learn a new word.</li>
              </ul>

              <p>Therefore, the goal is not simply "more Syntrons." The goal is to build systems where the dynamic interplay between a compact, powerful vocabulary and highly adaptive local policies leads to an emergent, efficient, and generalizable intelligence. The metric of success is not the final number of primitives, but the performance achieved per primitive and the system's ability to autonomously find the optimal level of complexity for a given task.</p>
            </div>

            <div className="text-base md:text-lg font-inter leading-relaxed max-w-3xl space-y-6 pt-8">
              <div className="bg-muted/30 border border-muted rounded-lg p-6" style={{ backgroundColor: 'rgba(42, 157, 143, 0.1)' }}>
                <h2 className="text-2xl sm:text-3xl mb-4" style={{ color: '#264653' }}>Research Paper and Code Available Soon</h2>
                <p className="text-muted-foreground" style={{ color: '#264653' }}>
                  The complete research paper and implementation code for Syntrons will be released soon.
                  Stay tuned for updates on this groundbreaking approach to artificial intelligence.
                </p>
              </div>
            </div>


          </div>
        </main>
        <MegaFooter />
      </div>
    </div>
  );
};

export default SyntronResearch;